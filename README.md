<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/logo.png?raw=true" alt="Logo" width="400"/>

## Trabajo Fin de Máster FP en IA y Big Data, CPIFP Malaga Tech Park

# SaveYourLife

https://saveyourlife.streamlit.app/

# Vídeo

[![SaveYourLife](https://img.youtube.com/vi/StTqXEQ2l-Y/0.jpg)](https://www.youtube.com/watch?v=WBsIK20DW74"SaveYourLife")

<a href=“https://www.youtube.com/watch?v=WBsIK20DW74”><img src=“https://img.youtube.com/vi/VIDEO_ID/maxresdefault.jpg” alt=“IMAGE ALT TEXT”></a>

https://www.youtube.com/watch?v=WBsIK20DW74

Trabajo realizado por Samuel Martínez Camacho y Miguel Angel Ortega Durán
## ↘️ Justificación y descripción del proyecto.
Pensamos que puede ser útil para quiénes buscan un lugar para vivir aquí en Málaga. De esta manera, el cliente podrá ver qué presupuesto necesitará para comprar o vender según el tipo de vivienda que esté buscando o vendiendo. Usamos datos reales para que el cliente tenga una estimación más acertada del precio.
Nos hemos decidido a realizar el proyecto fin de Master sobre la predicción de tumores cerebrales mediante el uso de redes neuronales. Utilizaremos técnicas de Deep Learning para entrenar una red neuronal con imágenes de MRI de pacientes previamente diagnosticados con tumores cerebrales. La red será capaz de predecir la presencia y tambien vamos a intentar que sea capaz de predecir el tipo de tumor en pacientes, a partir de sus imágenes de MRI. 
Este tipo de proyectos podrian tener un impacto positivo en el campo de la medicina, permitiendo una detección temprana y precisa de tumores cerebrales, mejorando el pronóstico y tratamiento para los pacientes.
## ↘️ Descripción.
El proyecto de Machine Learning constará de lo siguiente:
* Selección de una base de datos adecuada de imágenes de MRI de pacientes con tumores cerebrales previamente diagnosticados.
* Preprocesamiento de las imágenes para asegurar una correcta entrada a la red neuronal.
* Entrenamiento de la red neuronal con técnicas de Deep Learning utilizando la base de datos seleccionada.
* Validación del modelo entrenado mediante pruebas con imágenes de MRI de pacientes no incluidos en la base de datos de entrenamiento.
* Mejora del modelo y optimización de sus parámetros para lograr una mayor precisión en las predicciones.
* Integración de la aplicación de predicción de tumores cerebrales en una aplicacion Web.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/cerebro.png?raw=true" alt="Cerebro" width="400"/>

## RESUMEN

### Descipción del proyecto

El proyecto se enfoca en el uso de Machine Learning en el campo de la medicina para crear una herramienta de soporte para diagnóstico de tumores cerebrales mediante la detección de patrones en imágenes de resonancias magnéticas. Para ello, se seguirá un proceso que incluye la selección de una base de datos adecuada de imágenes de MRI de pacientes con tumores cerebrales, el preprocesamiento de las imágenes, el entrenamiento de una red neuronal con técnicas de Deep Learning, la validación del modelo entrenado mediante pruebas con imágenes de pacientes no incluidos en la base de datos de entrenamiento, la mejora del modelo y la optimización de sus parámetros para lograr una mayor precisión en las predicciones y la integración de la aplicación de predicción de tumores cerebrales en una aplicación web. 

El prototipo presentó una tasa de aciertos aceptable y se propusieron conclusiones y recomendaciones para futuros trabajos.

## INTRODUCCION

La tecnología ha producido herramientas innovadoras y tendencias para dar solución a las crecientes necesidades de empresas y consumidores. 

El Machine Learning (ML) es una subárea de la IA muy eficaz con éxito en diversas áreas del conocimiento, incluyendo la medicina. El almacenamiento electrónico de datos clínicos permite a la medicina ser el entorno preciso para desarrollar y aplicar nuevas herramientas tecnológicas. 

El ML puede analizar miles de datos e información clínica de los pacientes y crear modelos pronósticos, de tamizaje y diagnóstico, lo que puede mejorar la atención médica. Aunque son indiscutibles las ventajas de los algoritmos del ML para mejorar la atención a los pacientes, todavía se requiere del correcto proceso de evaluación para apreciar sus beneficios. 

Los profesionales de la salud ponen a disposición de sus pacientes mayor cantidad de terapias, medicinas, procedimientos médicos, y el procesamiento de tal cantidad de datos supera la mente humana. Por ello, se requiere la búsqueda de nuevas herramientas que ayuden a la integración de todos los datos de los pacientes, reconocimiento de patrones y creación de modelos que sirvan para solucionar las barreras de los doctores, reduciendo la carga de trabajo, mejorando y personalizando la atención, con el consiguiente ahorro de recursos. 

El proyecto de ML propuesto se enfoca en ayudar a los profesionales médicos que atienden a pacientes con tumores cerebrales a través de un modelo que permita detectar tumores cerebrales mediante imágenes de resonancias magnéticas para facilitar el diagnóstico del médico.

### Ubicación del Problema en un Contexto 

La tecnología aplicada a la ciencia médica se enfoca en la utilización de la inteligencia artificial (IA) y el aprendizaje automático (ML) para agilizar diagnósticos y ofrecer mejores tratamientos a los pacientes. 

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/inicio_2.jpeg?raw=true" alt="IA-Medical" width="400"/>


La IA y el ML son métodos, algoritmos y aplicaciones utilizados en el área médica para colaborar con los profesionales en su entorno y en investigaciones que se llevan a cabo. 

En particular, la IA y el ML pueden utilizarse para facilitar la detección de tumores cerebrales a través de imágenes de resonancias magnéticas. Aunque se desconoce si la utilización de modelos de ML para detectar tumores cerebrales es una realidad en los centros médicos, sería conveniente adoptar este tipo de modelos para mejorar los diagnósticos.

### CAUSAS Y CONSECUENCIAS DEL PROBLEMA 

La falta de un modelo de Machine Learning para la detección de tumores cerebrales puede llevar a que los profesionales de la salud no aprovechen las ventajas de esta tecnología, lo que podría retrasar los resultados de las imágenes y perjudicar el diagnóstico y tratamiento de los pacientes. 

El problema se delimita en el desarrollo de software en el área de Inteligencia Artificial, y su formulación busca automatizar el proceso de detección de tumores cerebrales para agilizar el diagnóstico médico. 

El proyecto es evidente, claro, relevante, original y factible, ya que busca mejorar la atención médica y es un campo poco analizado con antecedentes conocidos.

## OBJETIVOS

### Objetivo General 

Desarrollar un de modelo de ML que permita la automatización del proceso de detección de tumores cerebrales, y de identificar el tipo de tumor por medio de imágenes de resonancias magnéticas, para agilizar el diagnóstico del médico tratante. 

## Objetivos específicos

Realizar una revisión bibliográfica de las técnicas y metodologías utilizadas en el procesamiento de imágenes de resonancia magnética cerebral para la detección de tumores cerebrales.
Seleccionar y preprocesar un conjunto de imágenes de resonancia magnética cerebral para ser utilizado en el entrenamiento y evaluación del modelo de ML.
Diseñar e implementar un modelo de ML que permita la detección automática de tumores cerebrales a partir del conjunto de imágenes de resonancia magnética cerebral seleccionado.
Evaluar la precisión del modelo de ML y comparar los resultados con los diagnósticos realizados por profesionales médicos expertos en el tema.
Validar la efectividad del prototipo de modelo de ML desarrollado en un entorno de prueba clínico real y evaluar su capacidad para mejorar la eficiencia en la detección temprana de tumores cerebrales.

## JUSTIFICACIÓN

En resumen, la justificación e importancia del proyecto radican en la necesidad de brindar una atención médica de calidad y con los mejores diagnósticos y tratamientos para los pacientes. 

La inteligencia artificial y su subárea de Machine Learning son herramientas útiles en la ciencia médica y en particular para el diagnóstico temprano de enfermedades. 

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/inicio.jpeg?raw=true" alt="IA-Medical" width="400"/>


El modelo de ML para la detección de tumores cerebrales propuesto en este proyecto podría beneficiar a los profesionales médicos para un diagnóstico más ágil de estos problemas, con el fin de ofrecer mejor atención a los pacientes. Además, se utilizará una plataforma web y se emplearán datos públicos para su entrenamiento, lo que aumentará su accesibilidad y replicabilidad en otros entornos de salud.

## METODOLOGÍA

### Aprendizaje supervisado

En el proyecto de Machine Learning y Deep Learning para la detección de tumores cerebrales se utilizará el aprendizaje supervisado para entrenar el modelo. Este tipo de aprendizaje es adecuado para el proyecto ya que se dispone de un conjunto de datos etiquetados que permiten al modelo comprender cuál es la clasificación de cada imagen de resonancia magnética.

El modelo de ML se construirá utilizando algoritmos de redes neuronales, específicamente las librerías de Tensorflow y Keras, lo que permitirá una mayor precisión en la clasificación de las imágenes de resonancia magnética.

Se utilizarán diferentes set de datos públicos alojados en la página web de Kaggle para el entrenamiento del modelo de ML. El modelo presentará una interfaz para la carga de la imagen de la resonancia magnética y a través de un botón se podrá predecir si el paciente tiene o no un tumor cerebral con base en el modelo entrenado.

La importancia de este proyecto radica en que la IA y su subárea de ML son herramientas muy útiles en la ciencia médica, ya que ayudan en el diagnóstico temprano de enfermedades. Por tanto, el modelo de ML para la detección de tumores cerebrales propuesto en este proyecto podría beneficiar a los profesionales médicos para un diagnóstico más ágil de estos problemas, con el fin de ofrecer mejor atención a los pacientes.

### Flujo de trabajo de ML 

El flujo de trabajo de Machine Learning (ML) se compone de seis pasos: recolección, preprocesamiento, exploración, entrenamiento, evaluación y uso.

En la fase de recolección se obtienen los datos necesarios de distintas fuentes, lo que puede ser un proceso complejo que requiere tiempo y esfuerzo. Una vez que se dispone de los datos, se realiza la fase de preprocesamiento, donde se confirma que todos los datos estén en el formato correcto y se realizan tareas para preparar los datos para su uso en el algoritmo de aprendizaje.

La fase de exploración consiste en un análisis preliminar de los datos para identificar patrones, detectar valores atípicos y determinar las características que tienen mayor influencia en la predicción. Esto ayuda a la construcción del modelo y a mejorar su precisión.

En la fase de entrenamiento, los datos preprocesados se utilizan para alimentar el algoritmo de aprendizaje y extraer la información de mayor utilidad para realizar predicciones. Después, se procede a la fase de evaluación, donde se realizan pruebas para evaluar los resultados de las predicciones del modelo. Si los resultados no son satisfactorios, se regresa a la fase anterior para mejorar el modelo.

Finalmente, en la fase de uso, se pone en funcionamiento el modelo de ML para realizar las predicciones correspondientes en el contexto específico del proyecto.

Este flujo de trabajo se puede aplicar en diferentes proyectos de Machine Learning, como en la detección de tipos de tumores. Al seguir este proceso, se asegura que el modelo de ML se construya de manera óptima y pueda generar resultados precisos y útiles en el ámbito médico.

### Flujo de trabajo de DL 

El flujo de trabajo en Deep Learning es similar al del ML, pero con algunas diferencias. Los pasos son los siguientes:

Recolección de datos: Se recolectan datos de diversas fuentes, como bases de datos, archivos, imágenes, entre otros.

Preprocesamiento de datos: Se realiza una limpieza y normalización de los datos, y se dividen en conjuntos de entrenamiento, validación y prueba.

Diseño del modelo: Se selecciona la arquitectura de la red neuronal que se va a utilizar, incluyendo el número de capas y el tipo de neuronas.

Entrenamiento del modelo: Se alimenta la red neuronal con el conjunto de entrenamiento y se ajustan los pesos de las conexiones entre las neuronas para minimizar el error en las predicciones.

Validación del modelo: Se utiliza el conjunto de validación para ajustar los hiperparámetros del modelo, como la tasa de aprendizaje y el número de épocas de entrenamiento.

Evaluación del modelo: Se utiliza el conjunto de prueba para evaluar el desempeño del modelo en datos no vistos previamente.

Puesta en producción del modelo: Se utiliza el modelo para hacer predicciones en nuevos datos en tiempo real.

Es importante mencionar que el entrenamiento de modelos de Deep Learning requiere de una gran cantidad de datos y tiempo de cómputo, por lo que es común utilizar herramientas como GPUs o TPUs para acelerar el proceso. Además, el diseño y ajuste de la arquitectura de la red neuronal es un proceso iterativo y requiere de conocimientos avanzados en matemáticas y programación.

## METODOLOGÍA

### tipo de investigación

Para este proyecto se ha utilizado una metodología de investigación descriptiva, con el objetivo de identificar las características más relevantes de la construcción de modelos de Machine Learning adecuados para la detección de tumores cerebrales mediante resonancias magnéticas y el diseño del prototipo correspondiente. 

Se busca describir las tendencias de la población objeto de estudio y encontrar las características más relevantes del fenómeno analizado.

### Recoleccion de datos

En este proyecto, se ha utilizado el sitio web de Kaggle como una de las principales fuentes de datos para la recolección de información necesaria para el entrenamiento y desarrollo del modelo de detección de tumores cerebrales mediante resonancias magnéticas.

En esta la [fuente de datos 1](https://www.kaggle.com/datasets/abhranta/brain-tumor-detection-mri) se obtuvieron las imagenes necesrias para crear un primer modelo, capaz de detectar si el paciente tenia cáncer o no.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/no0.jpg?raw=true" alt="Sin tumor" width="200"/>
<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/y0.jpg?raw=true" alt="con tumor" width="200"/>

En la [fuente de datos 2](https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri) se obtuvieron imagnes para hacer un segundo modelo que fuese capaz de identificar el tipo de tumor o si no tenia tumor.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/tipo_glioma.jpg?raw=true" alt="Glioma" width="200"/>
<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/tipo_menignoma.jpg?raw=true" alt="Menignoma" width="200"/>

en la [fuente de datos 3](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset) se obtuvieron imagenes para hacer un tercer modelo que mejorara el rendimiento del modelo anterior.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/tipo_pituitario.jpg?raw=true" alt="pituitario" width="200"/>
<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/sin_tumor.jpg?raw=true" alt="Sin tumor" width="200"/>

y por ultimo en la [fuente de datos 4](https://www.mediafire.com/file/bv1wuu792sn9asb/Pictures_1-2000.rar/file) se obtuvieron imagenes de todotipo para poder hacer que el modelo discrimine entre imagenes random, e imagenes cerebrales de resonancia magnetica.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img_test_web/imagen_random.jpg?raw=true" alt="Random" width="200"/>

Al utilizar múltiples fuentes de datos, se ha aumentado la diversidad y cantidad de imágenes de resonancias magnéticas disponibles para el proyecto, lo que puede mejorar la precisión y eficacia del modelo de detección de tumores cerebrales.

### Construcción del algoritmo

### 1 Detección de tumores

El algoritmo que se propone en este proyecto se divide en varias etapas. Primero, se realiza una etapa de preprocesamiento de las imágenes de resonancia magnética para eliminar el ruido y mejorar la calidad de la imagen. Luego, se aplica una técnica de segmentación de imagen para identificar y resaltar las áreas anormales en las imágenes.

Después de la segmentación, se construye una Red Neuronal Convolucional (CNN) desde cero para clasificar las imágenes en dos categorías: imágenes de resonancia magnética de pacientes afectados por un tumor cerebral y las de sujetos sanos. La CNN se entrena utilizando un conjunto de datos de imágenes etiquetadas, y se ajusta con técnicas de regularización para evitar el sobreajuste.

Finalmente, se evalúa el modelo utilizando un conjunto de datos de prueba y se calculan las métricas de desempeño, como la precisión, la sensibilidad y la especificidad. El modelo final puede ser utilizado para ayudar a los profesionales de la salud en la detección temprana y el diagnóstico de tumores cerebrales a través del análisis de imágenes de resonancia magnética.

#### librerias que usamos para el proyecto 

Estas son las librerías que se utilizaron en este proyecto

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_import.png?raw=true" alt="import" width="400"/>

#### Extraccion y visualización de los datos

Cargamos los datos de imágenes de las carpetas (img_no, img_yes, img_pred) en la lista "sample" utilizando la función "glob" para cargar todos los archivos con extensión '.jpg' en el directorio especificado por "path". 

Luego, se crean tres variables que almacenan los datos cargados en cada una de las carpetas y se generan tres dataframes que etiquetan las imágenes con la clase "Sano" o "Afectado". Estos dataframes se combinan en uno solo llamado "train_data". 

Se divide el conjunto de datos de entrenamiento en dos partes usando la función "train_test_split" de la biblioteca "sklearn.model_selection". 

La variable "test_size" especifica la fracción de datos que se utilizarán para la validación y se establece en 10%, "shuffle" se establece en "True" para mezclar los datos antes de dividirlos y "random_state" se establece en 42 para garantizar que los mismos datos se dividan de la misma manera cada vez que se ejecute el código.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_set.png?raw=true" alt="set" width="400"/>

#### Costrucción de la red neuronal

Crear una estructura secuencial de capas, que pueden ser de diferentes tipos, como capas de extracción de características (Conv2D), reducción de resolución (MaxPooling2D), aplanamiento (Flatten), capas de clasificación (Dense) y capas de regularización (Dropout). 

Cada capa tiene diferentes parámetros que se ajustan durante el entrenamiento para mejorar la precisión de la red neuronal en la tarea específica que se está abordando. El resumen de la red neuronal muestra una descripción detallada de cada capa y sus parámetros.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_CNN.png?raw=true" alt="cnn" width="400"/>

#### Entrenamiento 

Para entrenar un modelo de red neuronal convolutional (CNN) en un conjunto de datos creamos una función en la que se especifican los conjuntos de datos de entrenamiento y validación, el optimizador, la función de pérdida y la métrica.

Además, se utilizan callbacks para monitorear el entrenamiento y mejorar la precisión del modelo. Después de especificar los parámetros, se llama a la función "fit" para entrenar el modelo. 

La función devuelve información sobre el entrenamiento, como la pérdida y la precisión en el conjunto de entrenamiento y validación a lo largo del tiempo, en un objeto "history".

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_fit.png?raw=true" alt="fit" width="400"/>

Utilizamos la técnica de validación cruzada. La función CV_training se encarga de entrenar y validar el modelo en diferentes subconjuntos de datos usando la función Model_fit.

Se establece el número de pliegues (k-fold) en 3 y el tamaño de la imagen en 224x224.

Utilizamos tambien el objeto ImageDataGenerator para escalar los valores de píxel de las imágenes y crear los lotes de datos de entrenamiento y validación.

Los parámetros especificados para el generador de datos incluyen el directorio donde se encuentran las imágenes, el nombre de la columna que contiene los nombres de archivo de las imágenes, la columna que contiene las etiquetas de clase, el tamaño de la imagen, el modo de color, el modo de clase, el tamaño del lote, si se deben mezclar los datos y la interpolación.

La función devuelve una lista de objetos History que se utilizan para graficar la evolución de la pérdida y la precisión durante el entrenamiento en cada partición del conjunto de datos.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_CV.png?raw=true" alt="cv" width="400"/>

#### Conclusiones y Resultado

En resumen, el entrenamiento del modelo de red neuronal convolutional utilizando las técnicas de EarlyStopping y validación cruzada ha producido un modelo de alta precisión que no sufre de sobreajuste y ha logrado una buena generalización de los datos de validación. La precisión obtenida en el conjunto de validación ha sido del 98%, lo que indica que el modelo es muy bueno en la tarea de clasificación de imágenes. Además, el valor de la pérdida de validación ha sido de 0.09, lo que indica que el modelo ha logrado una buena generalización de los datos de validación y que no ha sufrido de sobreajuste.

La utilización de la técnica de EarlyStopping ha sido fundamental para detener el entrenamiento del modelo cuando se ha alcanzado el mejor rendimiento en el conjunto de validación, evitando así el sobreajuste del modelo. La técnica de validación cruzada ha permitido evaluar el rendimiento del modelo en diferentes subconjuntos de datos y promediar los resultados para obtener una estimación más precisa del rendimiento del modelo en los datos completos. En conjunto, estas técnicas han permitido obtener un modelo de alta calidad que puede utilizarse para clasificar imágenes en una variedad de aplicaciones prácticas.

### 2 Detección de tipos tumores

Para entrenar este modelo, se han utilizado imágenes del cerebro de pacientes con diferentes tipos de corte de las imágenes, con el objetivo de que el modelo pueda determinar con cualquier imagen si se tiene tumor y de qué tipo. Los tipos de tumores que se identifican con este modelo son el glioma, el meningioma, el tumor pituitario y el cerebro sano sin tumor.

A diferencia del modelo anterior, en este caso se ha utilizado un modelo pre-entrenado previamente, que ya tenía una precisión bastante alta. Los datos de los tipos de tumor estudiados se han descargado de la web de la Clínica Mayo.

Es importante destacar que la detección temprana del tipo de tumor cerebral puede ser fundamental para el éxito del tratamiento del paciente. Si se logra identificar el tipo de tumor de manera temprana, se puede brindar un tratamiento más efectivo y personalizado al paciente, aumentando así las posibilidades de supervivencia y reduciendo el riesgo de complicaciones.

#### librerias que usamos para el proyecto 

Reutilizamos el modelo anterior cambiando ciertos hiperparametros.

#### Extraccion y visualización de los datos

Cargamos los datos de las imágenes de las diferentes carpetas correspondientes a cada tipo de tumor y a las imágenes sin tumor. Para ello, utiliza la función "glob" que carga todos los archivos con extensión '.jpg' que se encuentran en el directorio especificado por "path".

Los datos son cargados en la lista "sample" y luego son asignados a cinco variables que almacenan los datos de cada una de las carpetas. A continuación, se crean cinco dataframes que contienen la información sobre las imágenes cargadas y se etiquetan según la clase a la que pertenecen.

Por último, se combinan los dataframes correspondientes a cada tipo de tumor y a las imágenes sin tumor en un solo dataframe llamado "train_data". De esta manera, se pueden utilizar estos datos para entrenar el modelo de detección del tipo de tumor cerebral.

La variable "test_size" especifica la fracción de datos que se utilizarán para la validación y se establece en 10%, "shuffle" se establece en "True" para mezclar los datos antes de dividirlos y "random_state" se establece en 42 para garantizar que los mismos datos se dividan de la misma manera cada vez que se ejecute el código.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_set.png?raw=true" alt="set" width="400"/>

#### Costrucción de la red neuronal

Cargamos el modelo preentrenado anteriormente y luego lo modifica para realizar una tarea de clasificación de imágenes de tumores cerebrales. La ventaja de cargar un modelo preentrenado es que ya ha aprendido a extraer características de las imágenes, lo que puede ahorrar tiempo y mejorar la precisión de la tarea de clasificación. La función remueve las últimas seis capas del modelo preentrenado y las reemplaza con capas personalizadas de clasificación.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_2_CNN.png?raw=true" alt="cnn" width="400"/>

#### Entrenamiento 

Para entrenar un modelo de red neuronal convolutional (CNN) en un conjunto de datos creamos una función en la que se especifican los conjuntos de datos de entrenamiento y validación, el optimizador, la función de pérdida y la métrica.

Además, se utilizan callbacks para monitorear el entrenamiento y mejorar la precisión del modelo. Después de especificar los parámetros, se llama a la función "fit" para entrenar el modelo. 

La función devuelve información sobre el entrenamiento, como la pérdida y la precisión en el conjunto de entrenamiento y validación a lo largo del tiempo, en un objeto "history".

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_2_fit.png?raw=true" alt="fit" width="400"/>

Utilizamos la técnica de validación cruzada. La función CV_training se encarga de entrenar y validar el modelo en diferentes subconjuntos de datos usando la función Model_fit.

Se establece el número de pliegues (k-fold) en 3 y el tamaño de la imagen en 224x224.

Utilizamos tambien el objeto ImageDataGenerator para escalar los valores de píxel de las imágenes y crear los lotes de datos de entrenamiento y validación.

Los parámetros especificados para el generador de datos incluyen el directorio donde se encuentran las imágenes, el nombre de la columna que contiene los nombres de archivo de las imágenes, la columna que contiene las etiquetas de clase, el tamaño de la imagen, el modo de color, el modo de clase, el tamaño del lote, si se deben mezclar los datos y la interpolación.

La función devuelve una lista de objetos History que se utilizan para graficar la evolución de la pérdida y la precisión durante el entrenamiento en cada partición del conjunto de datos.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_CV.png?raw=true" alt="cv" width="400"/>

#### Conclusiones y Resultado

La inclusión del modelo pre-entrenado en la construcción del modelo ha permitido que la red neuronal pueda extraer características relevantes de las imágenes de los diferentes tipos de tumores cerebrales. Esto ha dado lugar a una mejora significativa en la precisión de la clasificación de los diferentes tipos de tumores con una precisión cercana al modelo anterior, incluso con menos capas y parámetros entrenables.

La utilización de un modelo pre-entrenado es una técnica comúnmente utilizada en el aprendizaje profundo ya que permite aprovechar el conocimiento previo de una red neuronal previamente entrenada en grandes conjuntos de datos. Esto ahorra tiempo y recursos, ya que no es necesario entrenar una red neuronal desde cero, lo que puede ser un proceso costoso y que requiere mucho tiempo.

En resumen, la inclusión del modelo pre-entrenado ha permitido mejorar la precisión de la clasificación de los diferentes tipos de tumores cerebrales con un menor costo computacional y de tiempo en el entrenamiento del modelo.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_2_CNN.png?raw=true" alt="cnn" width="400"/>

### 3 Discriminador mri

Después de entrenar los modelos anteriores, nos dimos cuenta de un problema que había que solucionar. La aplicación siempre intentaría realizar una predicción en el momento en que se cargara una imagen, lo cual podría llevar a intentar clasificar una imagen que no fuera de resonancia magnética.

Para resolver este problema, entrenamos un nuevo modelo utilizando imágenes tanto de resonancia magnética como imágenes aleatorias que descargamos. Basándonos en el primer modelo, este nuevo modelo fue entrenado para determinar si una imagen es una resonancia magnética o no.

Gracias a este nuevo modelo, nuestra aplicación ahora puede identificar si la imagen que se está analizando es una imagen de resonancia magnética o no.

#### librerias que usamos para el proyecto 

Estas son las librerías que se utilizaron en este proyecto

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_import.png?raw=true" alt="import" width="400"/>

#### Extraccion y visualización de los datos

Cargamos los datos de imágenes de las carpetas (img_mri, img_random, img_pred) en la lista "sample" utilizando la función "glob" para cargar todos los archivos con extensión '.jpg' en el directorio especificado por "path". 

Luego, se crean tres variables que almacenan los datos cargados en cada una de las carpetas y se generan tres dataframes que etiquetan las imágenes con la clase "mri" o "random". Estos dataframes se combinan en uno solo llamado "train_data". 

Se divide el conjunto de datos de entrenamiento en dos partes usando la función "train_test_split" de la biblioteca "sklearn.model_selection". 

La variable "test_size" especifica la fracción de datos que se utilizarán para la validación y se establece en 10%, "shuffle" se establece en "True" para mezclar los datos antes de dividirlos y "random_state" se establece en 42 para garantizar que los mismos datos se dividan de la misma manera cada vez que se ejecute el código.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_set.png?raw=true" alt="set" width="400"/>

#### Costrucción de la red neuronal

Crear una estructura secuencial de capas, que pueden ser de diferentes tipos, como capas de extracción de características (Conv2D), reducción de resolución (MaxPooling2D), aplanamiento (Flatten), capas de clasificación (Dense) y capas de regularización (Dropout). 

Cada capa tiene diferentes parámetros que se ajustan durante el entrenamiento para mejorar la precisión de la red neuronal en la tarea específica que se está abordando. El resumen de la red neuronal muestra una descripción detallada de cada capa y sus parámetros.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_CNN.png?raw=true" alt="cnn" width="400"/>

#### Entrenamiento 

Para entrenar un modelo de red neuronal convolutional (CNN) en un conjunto de datos creamos una función en la que se especifican los conjuntos de datos de entrenamiento y validación, el optimizador, la función de pérdida y la métrica.

Además, se utilizan callbacks para monitorear el entrenamiento y mejorar la precisión del modelo. Después de especificar los parámetros, se llama a la función "fit" para entrenar el modelo. 

La función devuelve información sobre el entrenamiento, como la pérdida y la precisión en el conjunto de entrenamiento y validación a lo largo del tiempo, en un objeto "history".

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_fit.png?raw=true" alt="fit" width="400"/>

Utilizamos la técnica de validación cruzada. La función CV_training se encarga de entrenar y validar el modelo en diferentes subconjuntos de datos usando la función Model_fit.

Se establece el número de pliegues (k-fold) en 3 y el tamaño de la imagen en 224x224.

Utilizamos tambien el objeto ImageDataGenerator para escalar los valores de píxel de las imágenes y crear los lotes de datos de entrenamiento y validación.

Los parámetros especificados para el generador de datos incluyen el directorio donde se encuentran las imágenes, el nombre de la columna que contiene los nombres de archivo de las imágenes, la columna que contiene las etiquetas de clase, el tamaño de la imagen, el modo de color, el modo de clase, el tamaño del lote, si se deben mezclar los datos y la interpolación.

La función devuelve una lista de objetos History que se utilizan para graficar la evolución de la pérdida y la precisión durante el entrenamiento en cada partición del conjunto de datos.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_1_CV.png?raw=true" alt="cv" width="400"/>

#### Conclusiones y Resultado

En conclusión, el modelo discriminador entrenado utilizando una red neuronal convolutional junto con técnicas de EarlyStopping y validación cruzada ha producido excelentes resultados. El modelo alcanzó una precisión del 100% en el conjunto de validación y una pérdida de validación de 0.000042275, lo que indica que el modelo es preciso en la tarea de clasificación de imágenes y que generaliza bien a datos desconocidos.

La utilización de la técnica de EarlyStopping ha sido clave para evitar el sobreajuste del modelo y detener el entrenamiento cuando se alcanzó el mejor rendimiento en el conjunto de validación. Además, la técnica de validación cruzada ha permitido evaluar el rendimiento del modelo en diferentes subconjuntos de datos y estimar su rendimiento en datos completos con mayor precisión.

En resumen, el modelo discriminador entrenado es de alta calidad y puede ser utilizado en diversas aplicaciones prácticas que requieren la clasificación precisa de imágenes. Los resultados obtenidos son prometedores y muestran el potencial de las redes neuronales convolutional y técnicas de entrenamiento como EarlyStopping y validación cruzada para la clasificación de imágenes.

### 4 chat_bot

El chatbot que hemos desarrollado es una herramienta sencilla que utiliza procesamiento del lenguaje natural para mejorar las respuestas a ciertas preguntas sobre los tumores que podemos detectar.

El chatbot está basado en una serie de patrones y respuestas que hemos elaborado en un archivo Json. Cuando el usuario hace una pregunta, el chatbot procesa la entrada del usuario utilizando técnicas de procesamiento del lenguaje natural para identificar la intención de la pregunta y luego proporciona una respuesta adecuada.

La incorporación de técnicas de procesamiento del lenguaje natural ha permitido al chatbot proporcionar respuestas más precisas y relevantes a las preguntas de los usuarios, mejorando así la experiencia general del usuario. Además, hemos incluido información precisa y actualizada sobre los tumores que podemos detectar para ayudar a los usuarios a comprender mejor esta enfermedad.

En resumen, el chatbot que hemos desarrollado es una herramienta útil que utiliza técnicas de procesamiento del lenguaje natural para mejorar las respuestas a preguntas sobre los tumores que podemos detectar. La inclusión de información precisa y actualizada sobre los tumores permite a los usuarios comprender mejor esta enfermedad y tomar decisiones informadas sobre su salud.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Chatbot.png?raw=true" alt="chatbot" width="400"/>

#### librerias que usamos para el proyecto 

Estas son las librerías que se utilizaron en este proyecto

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_4_import.png?raw=true" alt="Import" width="400"/>

#### Extraccion datos

La extracción de datos se realizó a partir de un archivo JSON que hemos realizado con patrones de entrada y sus etiquetas correspondientes. 

Se crearon variables para almacenar las palabras únicas, las etiquetas únicas y una lista de tuplas que contiene las palabras de los patrones y sus etiquetas. Se utilizó la biblioteca NLTK para llevar a cabo la lematización de las palabras en español, descargando el modelo de datos del WordNet en varios idiomas, incluido el español, para agrupar las palabras en conjuntos de sinónimos y proporcionar definiciones cortas. En general, se utilizó una variedad de técnicas y herramientas para extraer y procesar los datos de manera eficiente y efectiva.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_4_JSON.png?raw=true" alt="JSON" width="400"/>

#### Lematización

Este código carga un archivo JSON que contiene datos sobre diferentes intenciones de usuario y sus correspondientes respuestas. Luego, se procesan los datos para crear un modelo de clasificación de intenciones de usuario utilizando técnicas de procesamiento de lenguaje natural (NLP).

Primero, se crean tres variables: "words", "classes", y "documents". La variable "words" almacena todas las palabras únicas en los patrones de entrada; la variable "classes" almacena las etiquetas únicas de las intenciones; y la variable "documents" almacena una lista de tuplas, donde cada tupla contiene una lista de palabras para un patrón de entrada y su etiqueta correspondiente.

Luego, se lematizan las palabras, se ponen en minúscula, y se eliminan duplicados. 

Después, se ordenan las clases y se combinan los patrones e intenciones en una lista de tuplas.

Finalmente, se crea una función llamada "create_training_data" que procesa los documentos, las palabras, las clases y el lematizador para crear los datos de entrenamiento necesarios para entrenar el modelo de clasificación. 

Los datos de entrenamiento se crean utilizando una técnica llamada "bag of words", en la que se representa cada patrón de entrada como un vector de características, donde cada característica es una palabra en el vocabulario y el valor de cada característica indica si la palabra está presente en el patrón de entrada o no.

El código finaliza imprimiendo "Datos de entrenamiento creados", lo que indica que los datos de entrenamiento se han creado correctamente y están listos para ser utilizados para entrenar el modelo de clasificación.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_4_lema.png?raw=true" alt="Random" width="400"/>

#### Entrenamiento 

Definimos una función llamada create_model que crea un modelo de red neuronal con tres capas: una capa de entrada, una capa oculta y una capa de salida.

La capa de entrada tiene 128 neuronas y utiliza la función de activación ReLU. Esta capa se define con la función Dense de Keras, que crea una capa densa o completamente conectada.

La capa oculta también utiliza la función de activación ReLU y tiene 64 neuronas. Además, se aplica un dropout del 50% a esta capa, lo que significa que se apaga aleatoriamente la mitad de las neuronas de esta capa durante el entrenamiento, lo que ayuda a prevenir el sobreajuste.

La capa de salida utiliza la función de activación Softmax y tiene un número de neuronas igual al número de intenciones en el conjunto de datos de entrenamiento. La capa de salida produce una distribución de probabilidad que indica la probabilidad de que la entrada pertenezca a cada una de las intenciones posibles.

El código también crea una instancia del modelo llamado model y luego imprime los pesos de la primera capa del modelo usando la función get_weights(). Los pesos son inicializados de forma aleatoria antes de que el modelo sea entrenado.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_4_CNN.png?raw=true" alt="cnn" width="400"/>

definimos una función llamada Model_fit que entrena un modelo de red neuronal y lo guarda en un archivo llamado chatbot_model.h5. Luego, llama a esta función con los datos de entrenamiento train_x y train_y para entrenar el modelo y guarda el historial de entrenamiento en la variable history.

El modelo se compila con una función de pérdida de entropía cruzada categórica, un optimizador Adam y se mide su precisión.

La función fit se utiliza para entrenar el modelo con los datos de entrenamiento train_x y train_y. Se realiza un entrenamiento de 300 épocas con un tamaño de lote de 5.

Finalmente, el modelo entrenado se guarda en un archivo chatbot_model.h5 y se imprime un mensaje que indica que el modelo se ha creado y guardado correctamente. El historial de entrenamiento se devuelve como un resultado de la función y se asigna a la variable history.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_f4_fit.png?raw=true" alt="fit" width="400"/>

#### Conclusiones y Resultado

El chatbot desarrollado en este proyecto utiliza técnicas de procesamiento del lenguaje natural para proporcionar respuestas precisas a preguntas sobre tumores. 

Se extrajeron datos de un archivo JSON mediante la utilización de la biblioteca NLTK para la lematización de palabras en español. Se creó un modelo de clasificación de intenciones de usuario utilizando una técnica llamada "bag of words" y se entrenó utilizando una red neuronal con tres capas: una capa de entrada, una capa oculta y una capa de salida. 

El modelo se ha mejorado utilizando técnicas de preprocesamiento de datos para proporcionar respuestas más precisas y relevantes. En general, el chatbot es una herramienta útil que mejora la experiencia del usuario y ayuda a comprender mejor los tumores y tomar decisiones informadas sobre la salud.

### Implementación Web

#### Descripción de la implementación web en Streamlit

Para llevar a cabo la implementación de los algoritmos de detección de tumores, hemos desarrollado una aplicación web utilizando la herramienta Streamlit.

*La aplicación cuenta con una página de inicio y tres subpáginas a las que se puede acceder desde la página principal:

* Asistente Chatbot: una página en la que se puede interactuar con un chatbot que brinda información sobre la detección de tumores y responde preguntas frecuentes relacionadas con el tema.

* Detección de tumores: una página en la que se puede cargar una imagen médica para realizar la detección automática de tumores. La página muestra el resultado de la detección junto con estadísticas relevantes.

* Acerca de nosotros: una página que proporciona información sobre el equipo de desarrolladores y sus roles en el proyecto.

La aplicación web en Streamlit proporciona una interfaz fácil de usar para interactuar con los algoritmos de detección de tumores y obtener resultados precisos en cuestión de segundos.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_f4_fit.png?raw=true" alt="inicio" width="400"/>

### Detección de tumores

La página de tumores es una herramienta en línea diseñada para ayudar en la detección de tumores mediante la carga y procesamiento de imágenes médicas. Al subir una imagen médica, la página utiliza los algoritmos creados de procesamiento de imágenes para detectar la presencia de tumores en la imagen.

Esta herramienta es especialmente útil para profesionales médicos que buscan realizar un diagnóstico preciso y temprano de tumores en sus pacientes. También puede ser utilizada por individuos preocupados por su salud, que deseen monitorear la presencia de tumores en sus propios cuerpos.

La página de tumores es una solución innovadora y práctica para la detección temprana de tumores, lo que puede ayudar a salvar vidas mediante un diagnóstico oportuno y tratamiento adecuado.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_f4_fit.png?raw=true" alt="deteccion de tumores" width="400"/>

#### Asistente Chatbot

La página del asistente es una plataforma en línea que utiliza un chatbot para brindar asistencia y automatizada a los usuarios. El chatbot está diseñado con procesamiento del leguaje natural para entender las preguntas y solicitudes de los usuarios, y proporcionar respuestas útiles y relevantes en tiempo real.

Los usuarios pueden interactuar con el chatbot a través de la página web y hacer preguntas. El chatbot utiliza técnicas avanzadas de procesamiento de lenguaje natural para comprender la consulta y proporcionar una respuesta precisa y completa.

Además de responder preguntas, el chatbot también puede realizar tareas como consultas en la web.

La página del asistente es una solución práctica y conveniente para aquellos que necesitan una ayuda rápida y confiable en línea. 

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_f4_fit.png?raw=true" alt="chatbot" width="400"/>


### Acerca de nosotros

La última página creada es una sección de "Acerca de nosotros" (About Us) que describe com ha sido el proyecto detras de la pagina. La página proporciona información detallada sobre la visión y valores por los que hemos desarrollado este proyecto, así como detalles sobre su historia y logros.

La página de "Acerca de nosotros" es una parte importante del sitio web, ya que permite a los visitantes conocer más sobre el trabjo detrás de la página y comprender mejor su propósito y enfoque. También puede ayudar a establecer la confianza y credibilidad del proyecto, ya que proporciona información transparente y auténtica sobre quiénes son y qué hacen.

La página de "Acerca de nosotros" puede incluir secciones como "Nuestra historia", "Nuestro equipo", "Nuestra misión y visión", "Nuestros logros" y "Nuestros valores". Cada sección proporciona información importante que ayuda a los visitantes a entender la esencia de la organización y su propósito en el mundo.

En resumen, la página de "Acerca de nosotros" es esencial para cualquier sitio web que desee establecer una relación auténtica y confiable con sus visitantes, ya que brinda transparencia y contexto sobre la organización detrás de la página.

<img src="https://github.com/Smarcam/SaveYourLife/blob/main/img/img_doc/Captura_f4_fit.png?raw=true" alt="about us" width="400"/>
